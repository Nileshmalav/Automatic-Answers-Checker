{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54CPRXGfyg0h"
      },
      "source": [
        "#NECESSARY IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4hbFozwb2M5",
        "outputId": "dce45877-9cf0-4ce4-aae4-daecd3164a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/622.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m614.4/622.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.6/662.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCPU times: user 6.8 s, sys: 1.02 s, total: 7.82 s\n",
            "Wall time: 1min 20s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# For OCR\n",
        "%pip install -q autocorrect\n",
        "%pip install -q azure-cognitiveservices-vision-computervision\n",
        "%pip install -q pillow\n",
        "\n",
        "from autocorrect import Speller\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes, VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from getpass import getpass\n",
        "\n",
        "subscription_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "endpoint = \"https://nilesh.cognitiveservices.azure.com/\"\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
        "\n",
        "# For NLP\n",
        "%pip install -q gensim\n",
        "%pip install -q nltk\n",
        "%pip install -q sklearn\n",
        "%pip install -q tensorflow_hub\n",
        "%pip install -q pyemd\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "from pyemd import emd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBu3vV8fhQ"
      },
      "source": [
        "# OCR code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xK6zDLbmK95t"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Takes a list of uploaded filenames and returns list of file_text\n",
        "\"\"\"\n",
        "def get_ocr_text(uploaded_files):\n",
        "    to_return = []\n",
        "\n",
        "    if not uploaded_files:\n",
        "        print(\"No files uploaded!\")\n",
        "\n",
        "    check = Speller(lang='en')\n",
        "    for i, image in enumerate(uploaded_files, 1):\n",
        "        # Open the image\n",
        "        local_image_handwritten = open(image, \"rb\")\n",
        "\n",
        "        # Call API with image and raw response (allows you to get the operation location)\n",
        "        recognize_handwriting_results = computervision_client.read_in_stream(local_image_handwritten, raw=True)\n",
        "\n",
        "        # Get the operation location (URL with ID as last appendage)\n",
        "        operation_location_local = recognize_handwriting_results.headers[\"Operation-Location\"]\n",
        "        # Take the ID off and use to get results\n",
        "        operation_id_local = operation_location_local.split(\"/\")[-1]\n",
        "\n",
        "        # Call the \"GET\" API and wait for the retrieval of the results\n",
        "        while True:\n",
        "            recognize_handwriting_result = computervision_client.get_read_result(operation_id_local)\n",
        "            if recognize_handwriting_result.status not in ['notStarted', 'running']:\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "        # print(f\"===== Extracted text from image #{i}: =====\")\n",
        "        lines = []\n",
        "        # Print results, line by line\n",
        "        if recognize_handwriting_result.status == OperationStatusCodes.succeeded:\n",
        "            for text_result in recognize_handwriting_result.analyze_result.read_results:\n",
        "                for line in text_result.lines:\n",
        "                    # print(line.text, sep=' ')        # original OCR'ed line\n",
        "                    # only autocorrect words which aren't abbreviations.\n",
        "                    corrected = [word if bool(re.search(\"[A-Z]+\", word)) else check(word) for word in line.text.split() ]\n",
        "                    # print(\" \".join(corrected))\n",
        "                    lines.append(\" \".join(corrected))\n",
        "\n",
        "        to_return.append(\" \".join(lines))\n",
        "        # print()\n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpcNsdzSb4Hm"
      },
      "source": [
        "# NLP Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2BoN8HsegiN",
        "outputId": "33d441fe-b38f-4e76-a524-6a2d289e7f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:3: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KeyedVectors<vector_size=300, 3000000 keys>\n",
            "CPU times: user 7min 23s, sys: 40.8 s, total: 8min 4s\n",
            "Wall time: 11min 57s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "path_to_saved_model = gensim.downloader.load('word2vec-google-news-300', return_path=True)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_saved_model, binary=True)\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IVPJTPB2IVXW",
        "outputId": "35f22ff5-22df-4616-cfdd-2653edd01635"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path_to_saved_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUv8i9ONru5x",
        "outputId": "4ae225fd-0e69-45fb-927e-381038265482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 17.8 s, sys: 3.87 s, total: 21.7 s\n",
            "Wall time: 47.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bH5Ee_FmsAhp"
      },
      "outputs": [],
      "source": [
        "def cos_sim(input_vectors):\n",
        "    similarity = cosine_similarity(input_vectors)\n",
        "    return similarity\n",
        "\n",
        "negative = [\"not\" , \"without\", \"against\", \"bad\", \"useless\", \"no\", \"dislike\", \"hate\"]\n",
        "\n",
        "def semantic_similarity(actual_answer , given_answer) :\n",
        "    actual = actual_answer.lower().split(\".\")\n",
        "    given = given_answer.lower().split(\".\")\n",
        "\n",
        "    sim_checker = actual\n",
        "\n",
        "    not_matching_semantics = list()\n",
        "\n",
        "    semantic_1 = 0   # Actual_answer\n",
        "    semantic_2 = 0   # Given_answer\n",
        "\n",
        "    actual_embed_list = list()\n",
        "    given_embed_list = list()\n",
        "\n",
        "    for z in range(len(actual)) :\n",
        "        list_actual = list()\n",
        "        list_actual.append(actual[z])\n",
        "        actual_embed_list.append(embed(list_actual))\n",
        "        #print(actual_embed_list[z].shape)\n",
        "\n",
        "    for z in range(len(given)) :\n",
        "        semantic_1 = 0\n",
        "        semantic_2 = 0\n",
        "        list_given = list()\n",
        "        list_given.append(given[z])\n",
        "        embed_z = embed(list_given)\n",
        "\n",
        "        sim_check = sim_checker.copy()\n",
        "        sim_check.append(given[z])\n",
        "\n",
        "        sen_em = embed(sim_check)\n",
        "\n",
        "        similarity_matrix = cos_sim(np.array(sen_em))\n",
        "\n",
        "        similarity_matrix_df = pd.DataFrame(similarity_matrix)\n",
        "\n",
        "        cos_list = list(similarity_matrix_df[len(similarity_matrix_df) - 1])\n",
        "        cos_list = cos_list[:len(cos_list)-1]\n",
        "        #print(cos_list)\n",
        "\n",
        "        index = cos_list.index(max(cos_list))\n",
        "\n",
        "        actual_check = actual[index]\n",
        "        actual_check = actual_check.split()\n",
        "        for i in range(len(actual_check) - 1) :\n",
        "            if(actual_check[i] in negative and actual_check[i+1] in negative) :\n",
        "                semantic_1 += 1\n",
        "            elif(actual_check[i] in negative and actual_check[i+1] not in negative) :\n",
        "                semantic_1 -= 1\n",
        "\n",
        "        answer_given = given[z].split()\n",
        "        for i in range(len(answer_given) - 1) :\n",
        "            if(answer_given[i] in negative and answer_given[i+1] in negative) :\n",
        "                semantic_2 += 1\n",
        "            elif(answer_given[i] in negative and answer_given[i+1] not in negative) :\n",
        "                semantic_2 -= 1\n",
        "\n",
        "        if(semantic_1 == 0 and semantic_2 == 0) :\n",
        "\n",
        "            \"\"\"\n",
        "            Well and good\n",
        "            \"\"\"\n",
        "        elif(semantic_1 < 0  and semantic_2 >= 0) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "\n",
        "        elif(semantic_1 >= 0 and semantic_2 < 0 ) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "\n",
        "        #print(semantic_1,semantic_2,actual[index],given[z])\n",
        "        given_embed_list.append(embed_z)\n",
        "\n",
        "    #print(np.array(actual_embed_list).shape)\n",
        "    actual_embed = actual_embed_list[0]\n",
        "    #print(actual_embed.shape)\n",
        "\n",
        "    for i in range(len(actual_embed_list)-1) :\n",
        "        #print(actual_embed_list[i+1].shape)\n",
        "        actual_embed += actual_embed_list[i+1]\n",
        "\n",
        "    given_embed = given_embed_list[0]\n",
        "    for i in range(len(given_embed_list) - 1) :\n",
        "        given_embed += given_embed_list[i+1]\n",
        "\n",
        "    actual_embed = np.array(actual_embed).reshape(512)\n",
        "    given_embed = np.array(given_embed).reshape(512)\n",
        "    sem_checker = list([actual_embed,given_embed])\n",
        "    answer = pd.DataFrame(cos_sim(sem_checker))\n",
        "\n",
        "    return not_matching_semantics , answer[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V_oGCLP-tGGd"
      },
      "outputs": [],
      "source": [
        "def WMD(actual_answer, given_answer, model) :\n",
        "    actual_answer = actual_answer.lower().split()\n",
        "    actual_answer = [w for w in actual_answer if w not in stop_words]\n",
        "\n",
        "    given_answer = given_answer.lower().split()\n",
        "    given_answer = [w for w in given_answer if w not in stop_words]\n",
        "\n",
        "    return model.wmdistance(given_answer,actual_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HrGpKXAftR0O"
      },
      "outputs": [],
      "source": [
        "def score(given_answer, actual_answer, model) :\n",
        "    given_answer1 = given_answer[:]\n",
        "    actual_answer1 = actual_answer[:]\n",
        "\n",
        "    given_answer2 = given_answer[:]\n",
        "    actual_answer2 = actual_answer[:]\n",
        "\n",
        "    not_matching , similarity = semantic_similarity(actual_answer1, given_answer1)\n",
        "    distance = WMD(actual_answer2, given_answer2, model)\n",
        "\n",
        "    # if(similarity > 0) :\n",
        "    # if(distance == 0) :\n",
        "    #     return 1\n",
        "    print(\"NOT MATCHING TEXT: \", not_matching)\n",
        "    return similarity/distance\n",
        "    # else :\n",
        "        # return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnx_1xNeSS2U"
      },
      "source": [
        "# Run with Flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rYFnOGNS6nv",
        "outputId": "cd9ca909-00e8-4ff9-974c-eaea3d0c0b3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/220.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5\n",
            "/content\n",
            "fatal: destination path 'Automatic-Answer-checker-' already exists and is not an empty directory.\n",
            "/content/Automatic-Answer-checker-\n",
            "Currently in directory: /content/Automatic-Answer-checker-\n",
            "Path to saved model: /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
          ]
        }
      ],
      "source": [
        "%pip install -q flask\n",
        "%pip install -q flask_ngrok\n",
        "%pip install -q werkzeug\n",
        "%pip install -q flask_wtf\n",
        "!pip install --upgrade openai\n",
        "%cd /content/\n",
        "!git clone https://github.com/Jay22519/Automatic-Answer-checker-\n",
        "%cd Automatic-Answer-checker-\n",
        "!printf \"Currently in directory: \"; pwd\n",
        "\n",
        "print(\"Path to saved model:\", path_to_saved_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAcw6hNJSSL-",
        "outputId": "f5bf6b1a-5fa1-41d0-ffa0-24aa26274827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pot in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from pot) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from pot) (1.11.3)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.0.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-11-25T15:35:28+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To acces the Gloable link please click https://90a4-35-227-30-85.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:36] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:37] \"GET /static/assets/bootstrap/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:37] \"GET /static/assets/js/theme.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:37] \"GET /static/assets/js/jquery.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:37] \"GET /static/assets/bootstrap/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:37] \"GET /static/assets/fonts/ionicons.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:38] \"GET /static/assets/img/tech/image3.jpg HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:38] \"GET /static/assets/fonts/ionicons.ttf?v=2.0.0 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:40] \"GET /live-demo HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:41] \"\u001b[36mGET /static/assets/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:41] \"\u001b[36mGET /static/assets/fonts/ionicons.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:41] \"\u001b[36mGET /static/assets/js/theme.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:41] \"\u001b[36mGET /static/assets/bootstrap/js/bootstrap.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:41] \"\u001b[36mGET /static/assets/js/jquery.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:35:41] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******* Inside RESULT **********\n",
            "IT'S POST MALONE:  ImmutableMultiDict([('file1', <FileStorage: 'resume_1_october.pdf' ('application/pdf')>), ('file2', <FileStorage: 'resume_1_october.pdf' ('application/pdf')>)])\n",
            "Got files:  <FileStorage: 'resume_1_october.pdf' ('application/pdf')> [<FileStorage: 'resume_1_october.pdf' ('application/pdf')>]\n",
            "OCR READING\n",
            "Nilesh Malav Jhalawar, Rajasthan nileshmalav2020@gmail.com • 8000401652 • Portfolio • Linkedin • Github Education • B.Tech. in Computer Science 2020 - 2024 Motilal Nehru National Institute of Technology, Prayagraj CPI till 6th sem 7.54 • Class 12th 2019 - 2020 Shiv Jyoti Convent School, Kota, Rajasthan Percentage 89.8% • Class 10th 2017 - 2018 Shiv Jyoti Convent School, Kota, Rajasthan Percentage 91.2% Projects • TuruLob A Dating Social Media Website link Intro: A Dating Social Media Website made with Flask as backend with modern features. Technology Used: Flask as Backend, HTML, CSS, and JavaScript as Frontend. Features: Allows users to create profiles, share photos, and send messages to other users. Also includes features such as location-based matching and a chat system. • Movies Recommendation System link Intro: A recommendation system based on content filtering that suggests movies based on their likes and dislikes. Technology Used: Python, Flask as Backend, Content-Based Filtering Algorithm. Features: Recommends movies to users based on their likes and dislikes, watched movies, and watch again, and based on genre, language etc. • Mental Health Website link Intro: A project to track Mental Health and get advice based on symptoms with Django as Backend. Technology Used: Django as Backend, HTML, CSS, and JavaScript as Frontend. Features: Allows users to input symptoms to track their mental health and provides advice from mental health experts and request services like massage, mental advice etc. Skills • Python, C/C++, Java, OOP, DSA • Web Development, HTML, CSS, JavaScript , Django, Flask Achievements • Google Coding Competition Farewell Round Rank 967th • HackWithInfy Infosys Campus Ambassador • Microsoft Intern Engage 2022 Mentee 2022 • Specialist At CodeForces • JEE Mains 2020, AIR 2655 • JEE Advanced 2020, Rank 7121 Interests Cricket, GTA 5, Artificial Intelligence, Blockchain Technology,\n",
            "['Nilesh Malav Jhalawar, Rajasthan nileshmalav2020@gmail.com • 8000401652 • Portfolio • Linkedin • Github Education • B.Tech. in Computer Science 2020 - 2024 Motilal Nehru National Institute of Technology, Prayagraj CPI till 6th sem 7.54 • Class 12th 2019 - 2020 Shiv Jyoti Convent School, Kota, Rajasthan Percentage 89.8% • Class 10th 2017 - 2018 Shiv Jyoti Convent School, Kota, Rajasthan Percentage 91.2% Projects • TuruLob A Dating Social Media Website link Intro: A Dating Social Media Website made with Flask as backend with modern features. Technology Used: Flask as Backend, HTML, CSS, and JavaScript as Frontend. Features: Allows users to create profiles, share photos, and send messages to other users. Also includes features such as location-based matching and a chat system. • Movies Recommendation System link Intro: A recommendation system based on content filtering that suggests movies based on their likes and dislikes. Technology Used: Python, Flask as Backend, Content-Based Filtering Algorithm. Features: Recommends movies to users based on their likes and dislikes, watched movies, and watch again, and based on genre, language etc. • Mental Health Website link Intro: A project to track Mental Health and get advice based on symptoms with Django as Backend. Technology Used: Django as Backend, HTML, CSS, and JavaScript as Frontend. Features: Allows users to input symptoms to track their mental health and provides advice from mental health experts and request services like massage, mental advice etc. Skills • Python, C/C++, Java, OOP, DSA • Web Development, HTML, CSS, JavaScript , Django, Flask Achievements • Google Coding Competition Farewell Round Rank 967th • HackWithInfy Infosys Campus Ambassador • Microsoft Intern Engage 2022 Mentee 2022 • Specialist At CodeForces • JEE Mains 2020, AIR 2655 • JEE Advanced 2020, Rank 7121 Interests Cricket, GTA 5, Artificial Intelligence, Blockchain Technology,']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Exception on /result [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"<ipython-input-18-cacd0c2f649a>\", line 75, in result\n",
            "    resulting_scores = [my_score(teacher_text, stud ) for stud in students_texts]\n",
            "  File \"<ipython-input-18-cacd0c2f649a>\", line 75, in <listcomp>\n",
            "    resulting_scores = [my_score(teacher_text, stud ) for stud in students_texts]\n",
            "  File \"<ipython-input-18-cacd0c2f649a>\", line 66, in my_score\n",
            "    completion = client.chat.completions.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\", line 299, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 598, in create\n",
            "    return self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1063, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 842, in request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 873, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 933, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 873, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 933, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 885, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "INFO:werkzeug:127.0.0.1 - - [25/Nov/2023 15:36:06] \"\u001b[35m\u001b[1mPOST /result HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        }
      ],
      "source": [
        "!pip install pot\n",
        "from flask import Flask, redirect, render_template, request, make_response, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "from flask import Flask\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "# print(completion.choices[0].message)\n",
        "port_no = 5000\n",
        "app= Flask(__name__)\n",
        "ngrok. set_auth_token (\"2YabLee4pluYgzJYudeDrhq7XSL_6T7LZU5m2C7GaXTkSatEv\")\n",
        "public_url = ngrok.connect(port_no).public_url\n",
        "\n",
        "# app = Flask(__name__)\n",
        "# run_with_ngrok(app)\n",
        "\n",
        "UPLOAD_FOLDER = os.path.join(\"static\",\"assets\",\"img\", 'uploads')\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "app.config['JSON_SORT_KEYS'] = False\n",
        "\n",
        "def file_name(filename):\n",
        "    return os.path.join(UPLOAD_FOLDER, filename)\n",
        "\n",
        "@app.route(\"/\")\n",
        "@app.route(\"/index\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/live-demo\")\n",
        "def live_demo():\n",
        "    return render_template(\"live-demo.html\")\n",
        "\n",
        "@app.route(\"/result\", methods = ['GET', 'POST'])\n",
        "def result():\n",
        "    print(\"******* Inside RESULT **********\")\n",
        "    if request.method == 'POST':\n",
        "        print(\"IT'S POST MALONE: \", request.files)\n",
        "\n",
        "        teacher_file = request.files.get('file1', None)\n",
        "        student_files = request.files.getlist('file2', None)\n",
        "\n",
        "        print(\"Got files: \", teacher_file, student_files)\n",
        "\n",
        "        if teacher_file and student_files:\n",
        "            teacher_file.save(file_name(teacher_file.filename))\n",
        "\n",
        "            for file_ in student_files:\n",
        "                file_.save(file_name(file_.filename))\n",
        "\n",
        "            teacher_text = get_ocr_text([file_name(teacher_file.filename)])[0]\n",
        "            students_texts = get_ocr_text([file_name(f.filename) for f in student_files])\n",
        "            print(\"OCR READING\")\n",
        "            print(teacher_text)\n",
        "            print(students_texts)\n",
        "            OPENAI_API_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "            client = OpenAI(\n",
        "                 api_key=OPENAI_API_KEY\n",
        "            )\n",
        "\n",
        "            def my_score(teacher_text,students_texts):\n",
        "              completion = client.chat.completions.create(\n",
        "                  model=\"gpt-3.5-turbo\",\n",
        "                  messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a answer checker assistant, skilled in comparing student answers with teacher answers and give them score in scale of 10, you need to give int score only.\"},\n",
        "                    {\"role\": \"user\", \"content\": \"teacher answer : \"+teacher_text},\n",
        "                    {\"role\": \"user\", \"content\": \"student answer : \"+students_texts},\n",
        "                  ]\n",
        "              )\n",
        "              return completion\n",
        "            resulting_scores = [my_score(teacher_text, stud ) for stud in students_texts]\n",
        "            print(\"res is of length\", len(resulting_scores))\n",
        "\n",
        "            # return redirect(url_for(\"\"))\n",
        "            results_dict = {\n",
        "                \"Teacher Filepath\": file_name(teacher_file.filename),\n",
        "                \"Teacher Filename\": teacher_file.filename,\n",
        "                \"Student Grades\": [{\n",
        "                    \"Filepath\": file_name(f.filename),\n",
        "                    \"Filename\": f.filename,\n",
        "                    \"Score\": f\"{r:.4f}\",\n",
        "                } for f, r in zip(student_files, resulting_scores)],\n",
        "            }\n",
        "            return render_template(\"output.html\", result=results_dict)\n",
        "    return make_response(\"Invalid somehow!\")#redirect(url_for('home'))\n",
        "\n",
        "# @app.route(\"/results\")\n",
        "def output():\n",
        "    pass\n",
        "\n",
        "print(f\"To acces the Gloable link please click {public_url}\")\n",
        "app.run(port=port_no)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LGU__fjX0E9G"
      },
      "outputs": [],
      "source": [
        "# from flask import Flask\n",
        "# !pip install pyngrok\n",
        "# from pyngrok import ngrok\n",
        "# port_no = 5000\n",
        "# app= Flask(__name__)\n",
        "# ngrok. set_auth_token (\"2YabLee4pluYgzJYudeDrhq7XSL_6T7LZU5m2C7GaXTkSatEv\")\n",
        "# public_url = ngrok.connect(port_no).public_url\n",
        "# @app. route (\"/\")\n",
        "# def home():\n",
        "#  return f\"Running Flask on Google Colab!\"\n",
        "# print(f\"To acces the Gloable link please click {public_url}\")\n",
        "# app.run (port=port_no)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yJgZq1ByO5c3"
      },
      "outputs": [],
      "source": [
        "# %cd /content\n",
        "# %rm -r Automatic-Answer-checker-\n",
        "# %ls"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
