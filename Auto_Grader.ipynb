{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54CPRXGfyg0h"
      },
      "source": [
        "#NECESSARY IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4hbFozwb2M5",
        "outputId": "acb07bac-d34f-44dc-fdf2-1ad5a27e2702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/622.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m450.6/622.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.6/662.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.0.1.tar.gz (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.8/731.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-7.0.1-py3-none-any.whl size=21122 sha256=502d5a2b87d4b5765ac9e8895878fe5614ab4229d08545a8f5a0e5c1f4c81c1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/32/0e/27789b6fde02bf2b320d6f1a0fd9e1354b257c5f75eefc29bc\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.0.1\n",
            "Collecting pot\n",
            "  Downloading POT-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (789 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m790.0/790.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from pot) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from pot) (1.11.3)\n",
            "Installing collected packages: pot\n",
            "Successfully installed pot-0.9.1\n",
            "Collecting openai\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5\n",
            "CPU times: user 778 ms, sys: 135 ms, total: 913 ms\n",
            "Wall time: 1min 52s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# For OCR\n",
        "%pip install -q autocorrect\n",
        "%pip install -q azure-cognitiveservices-vision-computervision\n",
        "%pip install -q pillow\n",
        "\n",
        "\n",
        "# For NLP\n",
        "%pip install -q gensim\n",
        "%pip install -q nltk\n",
        "%pip install -q sklearn\n",
        "%pip install -q tensorflow_hub\n",
        "%pip install -q pyemd\n",
        "\n",
        "\n",
        "# For Flask Application\n",
        "# For Web Application\n",
        "%pip install -q flask\n",
        "%pip install -q flask_ngrok\n",
        "%pip install -q werkzeug\n",
        "%pip install -q flask_wtf\n",
        "%pip install pyngrok\n",
        "%pip install pot\n",
        "\n",
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6kmlrQGlFtZ",
        "outputId": "166b3d27-53b7-4a0e-d963-e320e50ae5e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# For OCR\n",
        "from autocorrect import Speller\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes, VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from getpass import getpass\n",
        "from setup import *\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(azure_key))\n",
        "\n",
        "\n",
        "# For NLP\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "from pyemd import emd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBu3vV8fhQ"
      },
      "source": [
        "# OCR code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK6zDLbmK95t"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Takes a list of uploaded filenames and returns list of file_text\n",
        "\"\"\"\n",
        "def get_ocr_text(uploaded_files):\n",
        "    to_return = []\n",
        "\n",
        "    if not uploaded_files:\n",
        "        print(\"No files uploaded!\")\n",
        "\n",
        "    check = Speller(lang='en')\n",
        "    for i, image in enumerate(uploaded_files, 1):\n",
        "        # Open the image\n",
        "        local_image_handwritten = open(image, \"rb\")\n",
        "\n",
        "        # Call API with image and raw response (allows you to get the operation location)\n",
        "        recognize_handwriting_results = computervision_client.read_in_stream(local_image_handwritten, raw=True)\n",
        "\n",
        "        # Get the operation location (URL with ID as last appendage)\n",
        "        operation_location_local = recognize_handwriting_results.headers[\"Operation-Location\"]\n",
        "        # Take the ID off and use to get results\n",
        "        operation_id_local = operation_location_local.split(\"/\")[-1]\n",
        "\n",
        "        # Call the \"GET\" API and wait for the retrieval of the results\n",
        "        while True:\n",
        "            recognize_handwriting_result = computervision_client.get_read_result(operation_id_local)\n",
        "            if recognize_handwriting_result.status not in ['notStarted', 'running']:\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "        # print(f\"===== Extracted text from image #{i}: =====\")\n",
        "        lines = []\n",
        "        # Print results, line by line\n",
        "        if recognize_handwriting_result.status == OperationStatusCodes.succeeded:\n",
        "            for text_result in recognize_handwriting_result.analyze_result.read_results:\n",
        "                for line in text_result.lines:\n",
        "                    # print(line.text, sep=' ')        # original OCR'ed line\n",
        "                    # only autocorrect words which aren't abbreviations.\n",
        "                    corrected = [word if bool(re.search(\"[A-Z]+\", word)) else check(word) for word in line.text.split() ]\n",
        "                    # print(\" \".join(corrected))\n",
        "                    lines.append(\" \".join(corrected))\n",
        "\n",
        "        to_return.append(\" \".join(lines))\n",
        "        # print()\n",
        "    return to_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpcNsdzSb4Hm"
      },
      "source": [
        "# NLP Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH5Ee_FmsAhp"
      },
      "outputs": [],
      "source": [
        "def cos_sim(input_vectors):\n",
        "    similarity = cosine_similarity(input_vectors)\n",
        "    return similarity\n",
        "\n",
        "negative = [\"not\" , \"without\", \"against\", \"bad\", \"useless\", \"no\", \"dislike\", \"hate\"]\n",
        "\n",
        "def semantic_similarity(actual_answer , given_answer) :\n",
        "    actual = actual_answer.lower().split(\".\")\n",
        "    given = given_answer.lower().split(\".\")\n",
        "\n",
        "    sim_checker = actual\n",
        "\n",
        "    not_matching_semantics = list()\n",
        "\n",
        "    semantic_1 = 0   # Actual_answer\n",
        "    semantic_2 = 0   # Given_answer\n",
        "\n",
        "    actual_embed_list = list()\n",
        "    given_embed_list = list()\n",
        "\n",
        "    for z in range(len(actual)) :\n",
        "        list_actual = list()\n",
        "        list_actual.append(actual[z])\n",
        "        actual_embed_list.append(embed(list_actual))\n",
        "\n",
        "    for z in range(len(given)) :\n",
        "        semantic_1 = 0\n",
        "        semantic_2 = 0\n",
        "        list_given = list()\n",
        "        list_given.append(given[z])\n",
        "        embed_z = embed(list_given)\n",
        "\n",
        "        sim_check = sim_checker.copy()\n",
        "        sim_check.append(given[z])\n",
        "\n",
        "        sen_em = embed(sim_check)\n",
        "\n",
        "        similarity_matrix = cos_sim(np.array(sen_em))\n",
        "\n",
        "        similarity_matrix_df = pd.DataFrame(similarity_matrix)\n",
        "\n",
        "        cos_list = list(similarity_matrix_df[len(similarity_matrix_df) - 1])\n",
        "        cos_list = cos_list[:len(cos_list)-1]\n",
        "\n",
        "        index = cos_list.index(max(cos_list))\n",
        "\n",
        "        actual_check = actual[index]\n",
        "        actual_check = actual_check.split()\n",
        "        for i in range(len(actual_check) - 1) :\n",
        "            if(actual_check[i] in negative and actual_check[i+1] in negative) :\n",
        "                semantic_1 += 1\n",
        "            elif(actual_check[i] in negative and actual_check[i+1] not in negative) :\n",
        "                semantic_1 -= 1\n",
        "\n",
        "        answer_given = given[z].split()\n",
        "        for i in range(len(answer_given) - 1) :\n",
        "            if(answer_given[i] in negative and answer_given[i+1] in negative) :\n",
        "                semantic_2 += 1\n",
        "            elif(answer_given[i] in negative and answer_given[i+1] not in negative) :\n",
        "                semantic_2 -= 1\n",
        "\n",
        "        if(semantic_1 == 0 and semantic_2 == 0) :\n",
        "\n",
        "            \"\"\"\n",
        "            Well and good\n",
        "            \"\"\"\n",
        "        elif(semantic_1 < 0  and semantic_2 >= 0) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "\n",
        "        elif(semantic_1 >= 0 and semantic_2 < 0 ) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "\n",
        "\n",
        "        given_embed_list.append(embed_z)\n",
        "    actual_embed = actual_embed_list[0]\n",
        "\n",
        "    for i in range(len(actual_embed_list)-1) :\n",
        "\n",
        "        actual_embed += actual_embed_list[i+1]\n",
        "\n",
        "    given_embed = given_embed_list[0]\n",
        "    for i in range(len(given_embed_list) - 1) :\n",
        "        given_embed += given_embed_list[i+1]\n",
        "\n",
        "    actual_embed = np.array(actual_embed).reshape(512)\n",
        "    given_embed = np.array(given_embed).reshape(512)\n",
        "    sem_checker = list([actual_embed,given_embed])\n",
        "    answer = pd.DataFrame(cos_sim(sem_checker))\n",
        "\n",
        "    return not_matching_semantics , answer[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_oGCLP-tGGd"
      },
      "outputs": [],
      "source": [
        "def WMD(actual_answer, given_answer, model) :\n",
        "    actual_answer = actual_answer.lower().split()\n",
        "    actual_answer = [w for w in actual_answer if w not in stop_words]\n",
        "\n",
        "    given_answer = given_answer.lower().split()\n",
        "    given_answer = [w for w in given_answer if w not in stop_words]\n",
        "\n",
        "    return model.wmdistance(given_answer,actual_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrGpKXAftR0O"
      },
      "outputs": [],
      "source": [
        "def score(given_answer, actual_answer, model) :\n",
        "    given_answer1 = given_answer[:]\n",
        "    actual_answer1 = actual_answer[:]\n",
        "\n",
        "    given_answer2 = given_answer[:]\n",
        "    actual_answer2 = actual_answer[:]\n",
        "\n",
        "    not_matching , similarity = semantic_similarity(actual_answer1, given_answer1)\n",
        "    distance = WMD(actual_answer2, given_answer2, model)\n",
        "\n",
        "    # if(similarity > 0) :\n",
        "    # if(distance == 0) :\n",
        "    #     return 1\n",
        "    print(\"NOT MATCHING TEXT: \", not_matching)\n",
        "    return similarity/distance\n",
        "    # else :\n",
        "        # return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnx_1xNeSS2U"
      },
      "source": [
        "# Run with Flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rYFnOGNS6nv",
        "outputId": "b740dfcc-2f6d-4cf5-c83e-8d3f8c8c8309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Automatic-Answers-Checker'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 66 (delta 14), reused 63 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (66/66), 4.85 MiB | 10.63 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "/content/Automatic-Answers-Checker\n",
            "Currently in directory: /content/Automatic-Answers-Checker\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-11-28T20:09:25+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/Nileshmalav/Automatic-Answers-Checker.git\n",
        "%cd Automatic-Answers-Checker\n",
        "!printf \"Currently in directory: \"; pwd\n",
        "\n",
        "# import flask and setup\n",
        "from flask import Flask, redirect, render_template, request, make_response, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "from flask import Flask\n",
        "from pyngrok import ngrok\n",
        "# from openai import OpenAI\n",
        "\n",
        "port_no = 5000\n",
        "app= Flask(__name__)\n",
        "ngrok. set_auth_token (\"2YabLee4pluYgzJYudeDrhq7XSL_6T7LZU5m2C7GaXTkSatEv\")\n",
        "public_url = ngrok.connect(port_no).public_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAcw6hNJSSL-",
        "outputId": "f0465d72-1164-40e5-fcf7-eb40646d14c2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To acces the Gloable link please click https://2815-34-80-139-67.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:50] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:51] \"GET /static/assets/bootstrap/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:51] \"GET /static/assets/js/theme.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:51] \"GET /static/assets/fonts/ionicons.min.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:51] \"GET /static/assets/main.css HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:51] \"GET /static/assets/js/jquery.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:53] \"GET /static/assets/fonts/ionicons.ttf?v=2.0.0 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:55] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:55] \"GET /index HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:56] \"\u001b[36mGET /static/assets/js/theme.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:56] \"\u001b[36mGET /static/assets/main.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:56] \"\u001b[36mGET /static/assets/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:56] \"\u001b[36mGET /static/assets/js/jquery.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:56] \"\u001b[36mGET /static/assets/fonts/ionicons.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:58] \"GET /index HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:59] \"\u001b[36mGET /static/assets/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:59] \"\u001b[36mGET /static/assets/main.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:59] \"\u001b[36mGET /static/assets/js/jquery.min.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:59] \"\u001b[36mGET /static/assets/js/theme.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:41:59] \"\u001b[36mGET /static/assets/fonts/ionicons.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:42:03] \"GET /live-demo HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:42:04] \"\u001b[36mGET /static/assets/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:42:04] \"\u001b[36mGET /static/assets/main.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:42:04] \"\u001b[36mGET /static/assets/js/theme.js HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:42:04] \"\u001b[36mGET /static/assets/fonts/ionicons.min.css HTTP/1.1\u001b[0m\" 304 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Nov/2023 20:42:04] \"\u001b[36mGET /static/assets/js/jquery.min.js HTTP/1.1\u001b[0m\" 304 -\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "UPLOAD_FOLDER = os.path.join(\"static\",\"assets\",\"img\", 'uploads')\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "app.config['JSON_SORT_KEYS'] = False\n",
        "\n",
        "def file_name(filename):\n",
        "    return os.path.join(UPLOAD_FOLDER, filename)\n",
        "\n",
        "@app.route(\"/\")\n",
        "@app.route(\"/index\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/live-demo\")\n",
        "def live_demo():\n",
        "    return render_template(\"live-demo.html\")\n",
        "\n",
        "@app.route(\"/result\", methods = ['GET', 'POST'])\n",
        "def result():\n",
        "\n",
        "    print(\"******* Inside RESULT **********\")\n",
        "    if request.method == 'POST':\n",
        "        print(\"IT'S POST MALONE: \", request.files)\n",
        "\n",
        "        teacher_file = request.files.get('file1', None)\n",
        "        student_files = request.files.getlist('file2', None)\n",
        "\n",
        "        print(\"Got files: \", teacher_file, student_files)\n",
        "\n",
        "        if teacher_file and student_files:\n",
        "            teacher_file.save(file_name(teacher_file.filename))\n",
        "\n",
        "            for file_ in student_files:\n",
        "                file_.save(file_name(file_.filename))\n",
        "\n",
        "            teacher_text = get_ocr_text([file_name(teacher_file.filename)])[0]\n",
        "            students_texts = get_ocr_text([file_name(f.filename) for f in student_files])\n",
        "            print(\"OCR READING\")\n",
        "            # print(teacher_text)\n",
        "            # print(students_texts)\n",
        "\n",
        "            resulting_scores = [my_score(teacher_text, stud ) for stud in students_texts]\n",
        "            print(\"res is of length\", len(resulting_scores))\n",
        "\n",
        "            # return redirect(url_for(\"\"))\n",
        "            results_dict = {\n",
        "                \"Teacher Filepath\": file_name(teacher_file.filename),\n",
        "                \"Teacher Filename\": teacher_file.filename,\n",
        "                \"Student Grades\": [{\n",
        "                    \"Filepath\": file_name(f.filename),\n",
        "                    \"Filename\": f.filename,\n",
        "                    \"Score\": r,\n",
        "                } for f, r in zip(student_files, resulting_scores)],\n",
        "            }\n",
        "            return render_template(\"output.html\", result=results_dict)\n",
        "    return make_response(\"Invalid somehow!\")#redirect(url_for('home'))\n",
        "\n",
        "# @app.route(\"/results\")\n",
        "def output():\n",
        "    pass\n",
        "\n",
        "print(f\"To acces the Gloable link please click {public_url}\")\n",
        "app.run(port=port_no)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJgZq1ByO5c3"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "%rm -r Automatic-Answers-Checker\n",
        "%ls"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}